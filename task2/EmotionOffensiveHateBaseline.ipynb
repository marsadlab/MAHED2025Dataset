{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdsrymQQUy45"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1750332066456,
     "user": {
      "displayName": "Fatema Khatun",
      "userId": "04694556593640634304"
     },
     "user_tz": -180
    },
    "id": "rC74jBpi63k8",
    "outputId": "04391a7a-8fa7-4b2b-dec9-bb0d3406b6e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class ArabicTextDataset(Dataset):\n",
    "    \"\"\"Custom dataset for Arabic text multi-task classification\"\"\"\n",
    "\n",
    "    def __init__(self, texts, emotions, offensive, hate, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.emotions = emotions\n",
    "        self.offensive = offensive\n",
    "        self.hate = hate\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "\n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'emotion': torch.tensor(self.emotions[idx], dtype=torch.long),\n",
    "            'offensive': torch.tensor(self.offensive[idx], dtype=torch.long),\n",
    "            'hate': torch.tensor(self.hate[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"Dataset class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1750332076717,
     "user": {
      "displayName": "Fatema Khatun",
      "userId": "04694556593640634304"
     },
     "user_tz": -180
    },
    "id": "wiD0LD3L68j0",
    "outputId": "85db9b3f-dada-411f-d615-d7b1f15b558e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-task model class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class MultiTaskAraBERT(nn.Module):\n",
    "    \"\"\"Multi-task classification model using AraBERTv2\"\"\"\n",
    "\n",
    "    def __init__(self, model_name, num_emotions, num_offensive, num_hate, dropout=0.3):\n",
    "        super(MultiTaskAraBERT, self).__init__()\n",
    "\n",
    "        # Load pre-trained AraBERT model\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Classification heads for each task\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "\n",
    "        self.emotion_classifier = nn.Linear(hidden_size, num_emotions)\n",
    "        self.offensive_classifier = nn.Linear(hidden_size, num_offensive)\n",
    "        self.hate_classifier = nn.Linear(hidden_size, num_hate)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Use pooled output (CLS token representation)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        # Get predictions for each task\n",
    "        emotion_logits = self.emotion_classifier(pooled_output)\n",
    "        offensive_logits = self.offensive_classifier(pooled_output)\n",
    "        hate_logits = self.hate_classifier(pooled_output)\n",
    "\n",
    "        return emotion_logits, offensive_logits, hate_logits\n",
    "\n",
    "print(\"Multi-task model class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 129,
     "status": "ok",
     "timestamp": 1750332130209,
     "user": {
      "displayName": "Fatema Khatun",
      "userId": "04694556593640634304"
     },
     "user_tz": -180
    },
    "id": "oGJtk9kg6_ER",
    "outputId": "07a17356-94d3-4141-e94b-e0843f0d0dbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main classifier class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Main Classifier Class\n",
    "# =============================\n",
    "\n",
    "class ArabicMultiTaskClassifier:\n",
    "    \"\"\"Main classifier class for Arabic multi-task text classification\"\"\"\n",
    "\n",
    "    def __init__(self, model_name='aubmindlab/bert-base-arabertv2', max_length=512):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Label encoders for each task\n",
    "        self.emotion_encoder = LabelEncoder()\n",
    "        self.offensive_encoder = LabelEncoder()\n",
    "        self.hate_encoder = LabelEncoder()\n",
    "\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        \"\"\"Load and preprocess data from Excel or CSV file\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "\n",
    "        # Determine file type and read accordingly\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_path.endswith(('.xlsx', '.xls')):\n",
    "            df = pd.read_excel(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Please use CSV (.csv) or Excel (.xlsx, .xls) files.\")\n",
    "\n",
    "        # Basic data info\n",
    "        print(f\"Dataset shape: {df.shape}\")\n",
    "        print(\"\\nColumn names:\", df.columns.tolist())\n",
    "        print(\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Check for missing values\n",
    "        print(\"\\nMissing values:\")\n",
    "        print(df.isnull().sum())\n",
    "\n",
    "        # Remove rows with missing text\n",
    "        df = df.dropna(subset=['text'])\n",
    "\n",
    "        # Fill missing labels with 'unknown' or most frequent value\n",
    "        df['Emotion'] = df['Emotion'].fillna('neutral')\n",
    "        df['Offensive'] = df['Offensive'].fillna('no')\n",
    "        df['Hate'] = df['Hate'].fillna('not_hate')\n",
    "\n",
    "        # Encode labels\n",
    "        df['emotion_encoded'] = self.emotion_encoder.fit_transform(df['Emotion'])\n",
    "        df['offensive_encoded'] = self.offensive_encoder.fit_transform(df['Offensive'])\n",
    "        df['hate_encoded'] = self.hate_encoder.fit_transform(df['Hate'])\n",
    "\n",
    "        # Print label distributions\n",
    "        print(\"\\nLabel distributions:\")\n",
    "        print(\"Emotions:\", df['Emotion'].value_counts())\n",
    "        print(\"Offensive:\", df['Offensive'].value_counts())\n",
    "        print(\"Hate:\", df['Hate'].value_counts())\n",
    "\n",
    "        return df\n",
    "\n",
    "    def prepare_data(self, df, test_size=0.2, val_size=0.1):\n",
    "        \"\"\"Prepare train, validation, and test datasets\"\"\"\n",
    "        print(\"Preparing datasets...\")\n",
    "\n",
    "        # Split data\n",
    "        train_df, test_df = train_test_split(\n",
    "            df, test_size=test_size, random_state=42, stratify=df['Emotion']\n",
    "        )\n",
    "\n",
    "        train_df, val_df = train_test_split(\n",
    "            train_df, test_size=val_size/(1-test_size), random_state=42, stratify=train_df['Emotion']\n",
    "        )\n",
    "\n",
    "        print(f\"Train size: {len(train_df)}\")\n",
    "        print(f\"Validation size: {len(val_df)}\")\n",
    "        print(f\"Test size: {len(test_df)}\")\n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset = ArabicTextDataset(\n",
    "            train_df['text'].values,\n",
    "            train_df['emotion_encoded'].values,\n",
    "            train_df['offensive_encoded'].values,\n",
    "            train_df['hate_encoded'].values,\n",
    "            self.tokenizer,\n",
    "            self.max_length\n",
    "        )\n",
    "\n",
    "        val_dataset = ArabicTextDataset(\n",
    "            val_df['text'].values,\n",
    "            val_df['emotion_encoded'].values,\n",
    "            val_df['offensive_encoded'].values,\n",
    "            val_df['hate_encoded'].values,\n",
    "            self.tokenizer,\n",
    "            self.max_length\n",
    "        )\n",
    "\n",
    "        test_dataset = ArabicTextDataset(\n",
    "            test_df['text'].values,\n",
    "            test_df['emotion_encoded'].values,\n",
    "            test_df['offensive_encoded'].values,\n",
    "            test_df['hate_encoded'].values,\n",
    "            self.tokenizer,\n",
    "            self.max_length\n",
    "        )\n",
    "\n",
    "        return train_dataset, val_dataset, test_dataset, train_df, val_df, test_df\n",
    "\n",
    "    def create_model(self, num_emotions, num_offensive, num_hate):\n",
    "        \"\"\"Create and initialize the multi-task model\"\"\"\n",
    "        print(\"Creating model...\")\n",
    "\n",
    "        model = MultiTaskAraBERT(\n",
    "            self.model_name,\n",
    "            num_emotions,\n",
    "            num_offensive,\n",
    "            num_hate\n",
    "        )\n",
    "\n",
    "        model.to(self.device)\n",
    "        return model\n",
    "\n",
    "    def train_model(self, model, train_loader, val_loader, num_epochs=5, learning_rate=2e-5):\n",
    "        \"\"\"Train the multi-task model\"\"\"\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "        # Loss functions for each task\n",
    "        criterion_emotion = nn.CrossEntropyLoss()\n",
    "        criterion_offensive = nn.CrossEntropyLoss()\n",
    "        criterion_hate = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Optimizer and scheduler\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "        total_steps = len(train_loader) * num_epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        # Training history\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            total_train_loss = 0\n",
    "\n",
    "            for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                emotion_labels = batch['emotion'].to(self.device)\n",
    "                offensive_labels = batch['offensive'].to(self.device)\n",
    "                hate_labels = batch['hate'].to(self.device)\n",
    "\n",
    "                # Forward pass\n",
    "                emotion_logits, offensive_logits, hate_logits = model(input_ids, attention_mask)\n",
    "\n",
    "                # Calculate losses\n",
    "                emotion_loss = criterion_emotion(emotion_logits, emotion_labels)\n",
    "                offensive_loss = criterion_offensive(offensive_logits, offensive_labels)\n",
    "                hate_loss = criterion_hate(hate_logits, hate_labels)\n",
    "\n",
    "                # Combined loss (weighted sum)\n",
    "                total_loss = emotion_loss + offensive_loss + hate_loss\n",
    "\n",
    "                # Backward pass\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                total_train_loss += total_loss.item()\n",
    "\n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "            train_losses.append(avg_train_loss)\n",
    "\n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                    input_ids = batch['input_ids'].to(self.device)\n",
    "                    attention_mask = batch['attention_mask'].to(self.device)\n",
    "                    emotion_labels = batch['emotion'].to(self.device)\n",
    "                    offensive_labels = batch['offensive'].to(self.device)\n",
    "                    hate_labels = batch['hate'].to(self.device)\n",
    "\n",
    "                    emotion_logits, offensive_logits, hate_logits = model(input_ids, attention_mask)\n",
    "\n",
    "                    emotion_loss = criterion_emotion(emotion_logits, emotion_labels)\n",
    "                    offensive_loss = criterion_offensive(offensive_logits, offensive_labels)\n",
    "                    hate_loss = criterion_hate(hate_logits, hate_labels)\n",
    "\n",
    "                    total_loss = emotion_loss + offensive_loss + hate_loss\n",
    "                    total_val_loss += total_loss.item()\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(val_loader)\n",
    "            val_losses.append(avg_val_loss)\n",
    "\n",
    "            print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        return train_losses, val_losses\n",
    "\n",
    "    def predict_text(self, model, text):\n",
    "        \"\"\"Predict labels for a single text\"\"\"\n",
    "        model.eval()\n",
    "\n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            emotion_logits, offensive_logits, hate_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            # Get predictions\n",
    "            emotion_pred = torch.argmax(emotion_logits, dim=1).item()\n",
    "            offensive_pred = torch.argmax(offensive_logits, dim=1).item()\n",
    "            hate_pred = torch.argmax(hate_logits, dim=1).item()\n",
    "\n",
    "            # Get probabilities\n",
    "            emotion_probs = torch.softmax(emotion_logits, dim=1)[0]\n",
    "            offensive_probs = torch.softmax(offensive_logits, dim=1)[0]\n",
    "            hate_probs = torch.softmax(hate_logits, dim=1)[0]\n",
    "\n",
    "        # Convert to original labels\n",
    "        emotion_label = self.emotion_encoder.inverse_transform([emotion_pred])[0]\n",
    "        offensive_label = self.offensive_encoder.inverse_transform([offensive_pred])[0]\n",
    "        hate_label = self.hate_encoder.inverse_transform([hate_pred])[0]\n",
    "\n",
    "        return {\n",
    "            'emotion': {\n",
    "                'label': emotion_label,\n",
    "                'confidence': emotion_probs[emotion_pred].item()\n",
    "            },\n",
    "            'offensive': {\n",
    "                'label': offensive_label,\n",
    "                'confidence': offensive_probs[offensive_pred].item()\n",
    "            },\n",
    "            'hate': {\n",
    "                'label': hate_label,\n",
    "                'confidence': hate_probs[hate_pred].item()\n",
    "            }\n",
    "        }\n",
    "\n",
    "print(\"Main classifier class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1750332178204,
     "user": {
      "displayName": "Fatema Khatun",
      "userId": "04694556593640634304"
     },
     "user_tz": -180
    },
    "id": "C0ShSqwR7MGH",
    "outputId": "80b09c35-762e-474c-e116-2c4ac4702a10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Training Function\n",
    "# =========================\n",
    "\n",
    "def train_arabic_classifier(data_file_path, num_epochs=1, batch_size=16, learning_rate=2e-5):\n",
    "    \"\"\"\n",
    "    Main training function for Arabic multi-task text classification\n",
    "\n",
    "    Args:\n",
    "        data_file_path: Path to training data (CSV or Excel)\n",
    "        num_epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "        learning_rate: Learning rate for optimizer\n",
    "\n",
    "    Returns:\n",
    "        classifier: Trained classifier object\n",
    "        model: Trained model\n",
    "        results: Training results and metrics\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"ARABIC MULTI-TASK TEXT CLASSIFICATION TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Initialize classifier\n",
    "    classifier = ArabicMultiTaskClassifier()\n",
    "\n",
    "    # Load data\n",
    "    df = classifier.load_data(data_file_path)\n",
    "\n",
    "    # Prepare datasets\n",
    "    train_dataset, val_dataset, test_dataset, train_df, val_df, test_df = classifier.prepare_data(df)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Get number of classes for each task\n",
    "    num_emotions = len(classifier.emotion_encoder.classes_)\n",
    "    num_offensive = len(classifier.offensive_encoder.classes_)\n",
    "    num_hate = len(classifier.hate_encoder.classes_)\n",
    "\n",
    "    print(f\"\\nNumber of emotion classes: {num_emotions}\")\n",
    "    print(f\"Number of offensive classes: {num_offensive}\")\n",
    "    print(f\"Number of hate classes: {num_hate}\")\n",
    "\n",
    "    # Create model\n",
    "    model = classifier.create_model(num_emotions, num_offensive, num_hate)\n",
    "\n",
    "    # Train model\n",
    "    train_losses, val_losses = classifier.train_model(\n",
    "        model, train_loader, val_loader, num_epochs=num_epochs, learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'arabic_multitask_model.pth')\n",
    "    print(\"\\nModel saved as 'arabic_multitask_model.pth'\")\n",
    "\n",
    "    # Return everything needed for evaluation\n",
    "    return {\n",
    "        'classifier': classifier,\n",
    "        'model': model,\n",
    "        'test_loader': test_loader,\n",
    "        'test_df': test_df,\n",
    "        'full_df': df,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses\n",
    "    }\n",
    "\n",
    "print(\"Training function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tH1ZKhna7Yoi"
   },
   "outputs": [],
   "source": [
    "# Cell 6: Evaluation Functions\n",
    "# ============================\n",
    "\n",
    "def calculate_detailed_metrics(emotion_true, emotion_pred, offensive_true, offensive_pred, hate_true, hate_pred):\n",
    "    \"\"\"Calculate detailed metrics for each task\"\"\"\n",
    "\n",
    "    # Emotion metrics\n",
    "    emotion_accuracy = accuracy_score(emotion_true, emotion_pred)\n",
    "    emotion_precision, emotion_recall, emotion_f1, _ = precision_recall_fscore_support(\n",
    "        emotion_true, emotion_pred, average='weighted', zero_division=0\n",
    "    )\n",
    "\n",
    "    # Offensive metrics\n",
    "    offensive_accuracy = accuracy_score(offensive_true, offensive_pred)\n",
    "    offensive_precision, offensive_recall, offensive_f1, _ = precision_recall_fscore_support(\n",
    "        offensive_true, offensive_pred, average='weighted', zero_division=0\n",
    "    )\n",
    "\n",
    "    # Hate metrics\n",
    "    hate_accuracy = accuracy_score(hate_true, hate_pred)\n",
    "    hate_precision, hate_recall, hate_f1, _ = precision_recall_fscore_support(\n",
    "        hate_true, hate_pred, average='weighted', zero_division=0\n",
    "    )\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_accuracy = (emotion_accuracy + offensive_accuracy + hate_accuracy) / 3\n",
    "    avg_precision = (emotion_precision + offensive_precision + hate_precision) / 3\n",
    "    avg_recall = (emotion_recall + offensive_recall + hate_recall) / 3\n",
    "    avg_f1 = (emotion_f1 + offensive_f1 + hate_f1) / 3\n",
    "\n",
    "    metrics = {\n",
    "        'emotion': {\n",
    "            'accuracy': emotion_accuracy,\n",
    "            'precision': emotion_precision,\n",
    "            'recall': emotion_recall,\n",
    "            'f1_score': emotion_f1\n",
    "        },\n",
    "        'offensive': {\n",
    "            'accuracy': offensive_accuracy,\n",
    "            'precision': offensive_precision,\n",
    "            'recall': offensive_recall,\n",
    "            'f1_score': offensive_f1\n",
    "        },\n",
    "        'hate': {\n",
    "            'accuracy': hate_accuracy,\n",
    "            'precision': hate_precision,\n",
    "            'recall': hate_recall,\n",
    "            'f1_score': hate_f1\n",
    "        },\n",
    "        'average': {\n",
    "            'accuracy': avg_accuracy,\n",
    "            'precision': avg_precision,\n",
    "            'recall': avg_recall,\n",
    "            'f1_score': avg_f1\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxtsX6Jh7cEV"
   },
   "outputs": [],
   "source": [
    "def print_detailed_results(emotion_true, emotion_pred, offensive_true, offensive_pred, hate_true, hate_pred, metrics):\n",
    "    \"\"\"Print detailed classification results\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED CLASSIFICATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Individual task results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EMOTION CLASSIFICATION\")\n",
    "    print(\"=\"*50)\n",
    "    print(classification_report(emotion_true, emotion_pred))\n",
    "    print(f\"Accuracy: {metrics['emotion']['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['emotion']['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['emotion']['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {metrics['emotion']['f1_score']:.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"OFFENSIVE LANGUAGE DETECTION\")\n",
    "    print(\"=\"*50)\n",
    "    print(classification_report(offensive_true, offensive_pred))\n",
    "    print(f\"Accuracy: {metrics['offensive']['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['offensive']['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['offensive']['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {metrics['offensive']['f1_score']:.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"HATE SPEECH DETECTION\")\n",
    "    print(\"=\"*50)\n",
    "    print(classification_report(hate_true, hate_pred))\n",
    "    print(f\"Accuracy: {metrics['hate']['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['hate']['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['hate']['recall']:.4f}\")\n",
    "    print(f\"F1-Score: {metrics['hate']['f1_score']:.4f}\")\n",
    "\n",
    "    # Summary table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"METRICS SUMMARY TABLE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Task':<20} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Emotion':<20} {metrics['emotion']['accuracy']:<12.4f} {metrics['emotion']['precision']:<12.4f} {metrics['emotion']['recall']:<12.4f} {metrics['emotion']['f1_score']:<12.4f}\")\n",
    "    print(f\"{'Offensive':<20} {metrics['offensive']['accuracy']:<12.4f} {metrics['offensive']['precision']:<12.4f} {metrics['offensive']['recall']:<12.4f} {metrics['offensive']['f1_score']:<12.4f}\")\n",
    "    print(f\"{'Hate':<20} {metrics['hate']['accuracy']:<12.4f} {metrics['hate']['precision']:<12.4f} {metrics['hate']['recall']:<12.4f} {metrics['hate']['f1_score']:<12.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'AVERAGE':<20} {metrics['average']['accuracy']:<12.4f} {metrics['average']['precision']:<12.4f} {metrics['average']['recall']:<12.4f} {metrics['average']['f1_score']:<12.4f}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 78,
     "status": "ok",
     "timestamp": 1750332230166,
     "user": {
      "displayName": "Fatema Khatun",
      "userId": "04694556593640634304"
     },
     "user_tz": -180
    },
    "id": "AyhdkVK27hT9",
    "outputId": "748f779f-5808-42a0-d305-496bdd9a985a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(classifier, model, test_loader, test_df):\n",
    "    \"\"\"Evaluate the model on test data with detailed metrics\"\"\"\n",
    "    print(\"Evaluating model...\")\n",
    "\n",
    "    model.eval()\n",
    "    all_emotion_preds = []\n",
    "    all_offensive_preds = []\n",
    "    all_hate_preds = []\n",
    "    all_emotion_labels = []\n",
    "    all_offensive_labels = []\n",
    "    all_hate_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            input_ids = batch['input_ids'].to(classifier.device)\n",
    "            attention_mask = batch['attention_mask'].to(classifier.device)\n",
    "\n",
    "            emotion_logits, offensive_logits, hate_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            # Get predictions\n",
    "            emotion_preds = torch.argmax(emotion_logits, dim=1)\n",
    "            offensive_preds = torch.argmax(offensive_logits, dim=1)\n",
    "            hate_preds = torch.argmax(hate_logits, dim=1)\n",
    "\n",
    "            all_emotion_preds.extend(emotion_preds.cpu().numpy())\n",
    "            all_offensive_preds.extend(offensive_preds.cpu().numpy())\n",
    "            all_hate_preds.extend(hate_preds.cpu().numpy())\n",
    "\n",
    "            all_emotion_labels.extend(batch['emotion'].numpy())\n",
    "            all_offensive_labels.extend(batch['offensive'].numpy())\n",
    "            all_hate_labels.extend(batch['hate'].numpy())\n",
    "\n",
    "    # Convert predictions back to original labels\n",
    "    emotion_pred_labels = classifier.emotion_encoder.inverse_transform(all_emotion_preds)\n",
    "    offensive_pred_labels = classifier.offensive_encoder.inverse_transform(all_offensive_preds)\n",
    "    hate_pred_labels = classifier.hate_encoder.inverse_transform(all_hate_preds)\n",
    "\n",
    "    emotion_true_labels = classifier.emotion_encoder.inverse_transform(all_emotion_labels)\n",
    "    offensive_true_labels = classifier.offensive_encoder.inverse_transform(all_offensive_labels)\n",
    "    hate_true_labels = classifier.hate_encoder.inverse_transform(all_hate_labels)\n",
    "\n",
    "    # Calculate detailed metrics\n",
    "    metrics = calculate_detailed_metrics(\n",
    "        emotion_true_labels, emotion_pred_labels,\n",
    "        offensive_true_labels, offensive_pred_labels,\n",
    "        hate_true_labels, hate_pred_labels\n",
    "    )\n",
    "\n",
    "    # Print detailed results\n",
    "    print_detailed_results(\n",
    "        emotion_true_labels, emotion_pred_labels,\n",
    "        offensive_true_labels, offensive_pred_labels,\n",
    "        hate_true_labels, hate_pred_labels,\n",
    "        metrics\n",
    "    )\n",
    "\n",
    "    # Create results DataFrame with predictions\n",
    "    results_df = test_df.copy()\n",
    "    results_df['emotion_predicted'] = emotion_pred_labels\n",
    "    results_df['offensive_predicted'] = offensive_pred_labels\n",
    "    results_df['hate_predicted'] = hate_pred_labels\n",
    "\n",
    "    # Add correctness indicators\n",
    "    results_df['emotion_correct'] = results_df['Emotion'] == results_df['emotion_predicted']\n",
    "    results_df['offensive_correct'] = results_df['Offensive'] == results_df['offensive_predicted']\n",
    "    results_df['hate_correct'] = results_df['Hate'] == results_df['hate_predicted']\n",
    "\n",
    "    # Save results to file\n",
    "    results_df.to_excel('test_predictions_with_original_data.xlsx', index=False)\n",
    "    print(f\"\\nTest predictions saved to 'test_predictions_with_original_data.xlsx'\")\n",
    "\n",
    "    return {\n",
    "        'metrics': metrics,\n",
    "        'results_df': results_df\n",
    "    }\n",
    "\n",
    "print(\"Evaluation functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 75,
     "status": "ok",
     "timestamp": 1750332284117,
     "user": {
      "displayName": "Fatema Khatun",
      "userId": "04694556593640634304"
     },
     "user_tz": -180
    },
    "id": "WbKaOwCc7khB",
    "outputId": "7d8ebbaf-bf76-43d8-bbcf-16c2f129e9cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External test file prediction functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: External Test File Prediction and Evaluation\n",
    "# ===================================================\n",
    "\n",
    "def predict_on_external_test_file(classifier, model, test_file_path):\n",
    "    \"\"\"\n",
    "    Predict on an external test CSV file provided by the user\n",
    "\n",
    "    Args:\n",
    "        classifier: Trained classifier object\n",
    "        model: Trained model\n",
    "        test_file_path: Path to external test CSV/Excel file\n",
    "\n",
    "    Returns:\n",
    "        results_df: DataFrame with predictions and evaluations\n",
    "    \"\"\"\n",
    "    print(f\"Loading external test file: {test_file_path}\")\n",
    "\n",
    "    # Load the external test file\n",
    "    if test_file_path.endswith('.csv'):\n",
    "        test_df = pd.read_csv(test_file_path)\n",
    "    elif test_file_path.endswith(('.xlsx', '.xls')):\n",
    "        test_df = pd.read_excel(test_file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please use CSV (.csv) or Excel (.xlsx, .xls) files.\")\n",
    "\n",
    "    print(f\"External test file loaded with {len(test_df)} samples\")\n",
    "    print(\"Columns in test file:\", test_df.columns.tolist())\n",
    "\n",
    "    # Check if 'text' column exists\n",
    "    if 'text' not in test_df.columns:\n",
    "        raise ValueError(\"Test file must contain a 'text' column\")\n",
    "\n",
    "    # Check if ground truth labels exist\n",
    "    has_labels = all(col in test_df.columns for col in ['Emotion', 'Offensive', 'Hate'])\n",
    "\n",
    "    if has_labels:\n",
    "        print(\"Ground truth labels found. Will calculate accuracy metrics.\")\n",
    "        # Encode labels if they exist\n",
    "        try:\n",
    "            test_df['emotion_encoded'] = classifier.emotion_encoder.transform(test_df['Emotion'])\n",
    "            test_df['offensive_encoded'] = classifier.offensive_encoder.transform(test_df['Offensive'])\n",
    "            test_df['hate_encoded'] = classifier.hate_encoder.transform(test_df['Hate'])\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: Some labels in test file are not seen during training: {e}\")\n",
    "            print(\"Proceeding with prediction only...\")\n",
    "            has_labels = False\n",
    "    else:\n",
    "        print(\"No ground truth labels found. Proceeding with prediction only...\")\n",
    "\n",
    "    # Get predictions for all samples\n",
    "    model.eval()\n",
    "    all_texts = test_df['text'].tolist()\n",
    "    all_predictions = []\n",
    "\n",
    "    print(\"Making predictions...\")\n",
    "    for i, text in enumerate(tqdm(all_texts, desc=\"Predicting\")):\n",
    "        try:\n",
    "            prediction = classifier.predict_text(model, text)\n",
    "            all_predictions.append(prediction)\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting text at index {i}: {e}\")\n",
    "            # Add default prediction for failed cases\n",
    "            all_predictions.append({\n",
    "                'emotion': {'label': 'unknown', 'confidence': 0.0},\n",
    "                'offensive': {'label': 'unknown', 'confidence': 0.0},\n",
    "                'hate': {'label': 'unknown', 'confidence': 0.0}\n",
    "            })\n",
    "\n",
    "    # Create results DataFrame\n",
    "    results_df = test_df.copy()\n",
    "    results_df['emotion_predicted'] = [pred['emotion']['label'] for pred in all_predictions]\n",
    "    results_df['emotion_confidence'] = [pred['emotion']['confidence'] for pred in all_predictions]\n",
    "    results_df['offensive_predicted'] = [pred['offensive']['label'] for pred in all_predictions]\n",
    "    results_df['offensive_confidence'] = [pred['offensive']['confidence'] for pred in all_predictions]\n",
    "    results_df['hate_predicted'] = [pred['hate']['label'] for pred in all_predictions]\n",
    "    results_df['hate_confidence'] = [pred['hate']['confidence'] for pred in all_predictions]\n",
    "\n",
    "    # Calculate accuracy if ground truth is available\n",
    "    if has_labels:\n",
    "        results_df['emotion_correct'] = results_df['Emotion'] == results_df['emotion_predicted']\n",
    "        results_df['offensive_correct'] = results_df['Offensive'] == results_df['offensive_predicted']\n",
    "        results_df['hate_correct'] = results_df['Hate'] == results_df['hate_predicted']\n",
    "\n",
    "        # Calculate and print accuracies\n",
    "        emotion_accuracy = (results_df['emotion_correct'].sum() / len(results_df)) * 100\n",
    "        offensive_accuracy = (results_df['offensive_correct'].sum() / len(results_df)) * 100\n",
    "        hate_accuracy = (results_df['hate_correct'].sum() / len(results_df)) * 100\n",
    "        avg_accuracy = (emotion_accuracy + offensive_accuracy + hate_accuracy) / 3\n",
    "\n",
    "        print(f\"\\nExternal Test File Accuracy:\")\n",
    "        print(f\"Emotion: {emotion_accuracy:.2f}%\")\n",
    "        print(f\"Offensive: {offensive_accuracy:.2f}%\")\n",
    "        print(f\"Hate: {hate_accuracy:.2f}%\")\n",
    "        print(f\"Average: {avg_accuracy:.2f}%\")\n",
    "\n",
    "        # Calculate detailed metrics if labels are available\n",
    "        try:\n",
    "            # Calculate detailed metrics\n",
    "            metrics = calculate_detailed_metrics(\n",
    "                results_df['Emotion'], results_df['emotion_predicted'],\n",
    "                results_df['Offensive'], results_df['offensive_predicted'],\n",
    "                results_df['Hate'], results_df['hate_predicted']\n",
    "            )\n",
    "\n",
    "            # Print detailed results\n",
    "            print_detailed_results(\n",
    "                results_df['Emotion'], results_df['emotion_predicted'],\n",
    "                results_df['Offensive'], results_df['offensive_predicted'],\n",
    "                results_df['Hate'], results_df['hate_predicted'],\n",
    "                metrics\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating detailed metrics: {e}\")\n",
    "\n",
    "    # Save results\n",
    "    output_filename = f\"external_test_predictions_{test_file_path.split('/')[-1].split('.')[0]}.xlsx\"\n",
    "    results_df.to_excel(output_filename, index=False)\n",
    "    print(f\"\\nPredictions saved to '{output_filename}'\")\n",
    "\n",
    "    # Show sample predictions\n",
    "    show_sample_predictions(results_df, has_labels, num_samples=5)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "def show_sample_predictions(results_df, has_labels, num_samples=5):\n",
    "    \"\"\"Show sample predictions from the results\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAMPLE PREDICTIONS FROM EXTERNAL TEST FILE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Select random samples\n",
    "    sample_indices = random.sample(range(len(results_df)), min(num_samples, len(results_df)))\n",
    "\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        row = results_df.iloc[idx]\n",
    "        text = str(row['text'])\n",
    "\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Text: {text[:100]}{'...' if len(text) > 100 else ''}\")\n",
    "\n",
    "        if has_labels:\n",
    "            print(f\"TRUE LABELS  -> Emotion: {row['Emotion']}, Offensive: {row['Offensive']}, Hate: {row['Hate']}\")\n",
    "\n",
    "        print(f\"PREDICTIONS -> Emotion: {row['emotion_predicted']} (conf: {row['emotion_confidence']:.3f})\")\n",
    "        print(f\"               Offensive: {row['offensive_predicted']} (conf: {row['offensive_confidence']:.3f})\")\n",
    "        print(f\"               Hate: {row['hate_predicted']} (conf: {row['hate_confidence']:.3f})\")\n",
    "\n",
    "        if has_labels:\n",
    "            emotion_correct = \"‚úì\" if row['emotion_predicted'] == row['Emotion'] else \"‚úó\"\n",
    "            offensive_correct = \"‚úì\" if row['offensive_predicted'] == row['Offensive'] else \"‚úó\"\n",
    "            hate_correct = \"‚úì\" if row['hate_predicted'] == row['Hate'] else \"‚úó\"\n",
    "            print(f\"CORRECTNESS -> Emotion: {emotion_correct}, Offensive: {offensive_correct}, Hate: {hate_correct}\")\n",
    "\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "print(\"External test file prediction functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 441679,
     "status": "ok",
     "timestamp": 1750332916628,
     "user": {
      "displayName": "Fatema Khatun",
      "userId": "04694556593640634304"
     },
     "user_tz": -180
    },
    "id": "okIhS-Yk7xry",
    "outputId": "98862cb7-9c7b-490d-beaa-6248140ece50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ARABIC MULTI-TASK TEXT CLASSIFICATION TRAINING\n",
      "============================================================\n",
      "Using device: cuda\n",
      "Loading data...\n",
      "Dataset shape: (5960, 5)\n",
      "\n",
      "Column names: ['id', 'text', 'Emotion', 'Offensive', 'Hate']\n",
      "\n",
      "First few rows:\n",
      "     id                                               text       Emotion  \\\n",
      "0  2537  ÿ£ÿ≠ÿØ ÿßŸÑÿ™ÿ¨ÿßÿ± ÿßŸÑÿ¥ÿ®ÿßÿ® ÿßŸÑÿπŸÖÿßŸÜŸäŸäŸÜ ŸäŸÇŸàŸÑ ŸÑŸÑÿßÿ≥ŸÅ ŸÑŸÖÿß ŸäŸÉŸà...       neutral   \n",
      "1  5579  @JALHARBISKY ŸÖÿ¨ŸÖŸàÿπŸá ÿßŸÑŸÇÿØÿ±ÿ© ÿßŸÑÿ¨ŸÜÿ≥ŸäŸáüëç<LF> <LF>ÿ®ÿØ...      optimism   \n",
      "2  6092        @rwn4o ÿ≠ÿ®Ÿäÿ®ŸäŸäŸä ŸàÿßŸÑŸÑŸá ÿßŸÉÿ´ÿ´ÿ´ÿ±ÿ± Ÿäÿßÿ±ÿ® ÿßŸÖŸäŸÜü•∫‚ô•Ô∏è‚ô•Ô∏è          love   \n",
      "3  2540  #ŸàÿµÿßŸÑ_ÿØŸàÿ™_FM<LF>ŸÖÿπ ÿ≥ŸÖŸäÿ±ÿ© ÿßŸÑŸÅÿ∑Ÿäÿ≥Ÿäÿ© @Samira_Alfu...       neutral   \n",
      "4  3159  ŸÖŸÜ ŸäŸÜÿ™ÿ≤ÿπ ÿßÿ±Ÿàÿßÿ≠ ÿßÿ∑ŸÅÿßŸÑŸÜÿß ŸÖŸÜ ÿ£ÿ¨ÿ≥ÿßÿØŸáÿß ÿ®ŸÉŸÑ Ÿàÿ≠ÿ¥Ÿäÿ© ÿπŸÑ...  anticipation   \n",
      "\n",
      "  Offensive Hate  \n",
      "0        no  NaN  \n",
      "1        no  NaN  \n",
      "2        no  NaN  \n",
      "3        no  NaN  \n",
      "4        no  NaN  \n",
      "\n",
      "Missing values:\n",
      "id              0\n",
      "text            0\n",
      "Emotion         0\n",
      "Offensive       0\n",
      "Hate         4216\n",
      "dtype: int64\n",
      "\n",
      "Label distributions:\n",
      "Emotions: Emotion\n",
      "anger           1551\n",
      "disgust          777\n",
      "neutral          661\n",
      "love             593\n",
      "joy              533\n",
      "anticipation     491\n",
      "optimism         419\n",
      "sadness          335\n",
      "confidence       210\n",
      "pessimism        194\n",
      "surprise         143\n",
      "fear              53\n",
      "Name: count, dtype: int64\n",
      "Offensive: Offensive\n",
      "no     4216\n",
      "yes    1744\n",
      "Name: count, dtype: int64\n",
      "Hate: Hate\n",
      "not_hate    5657\n",
      "hate         303\n",
      "Name: count, dtype: int64\n",
      "Preparing datasets...\n",
      "Train size: 4172\n",
      "Validation size: 596\n",
      "Test size: 1192\n",
      "\n",
      "Number of emotion classes: 12\n",
      "Number of offensive classes: 2\n",
      "Number of hate classes: 2\n",
      "Creating model...\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 261/261 [06:22<00:00,  1.47s/it]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 38/38 [00:17<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 2.6509\n",
      "Average validation loss: 2.4087\n",
      "\n",
      "Model saved as 'arabic_multitask_model.pth'\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75/75 [00:35<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DETAILED CLASSIFICATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "==================================================\n",
      "EMOTION CLASSIFICATION\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.49      0.92      0.64       310\n",
      "anticipation       0.33      0.03      0.06        98\n",
      "  confidence       0.00      0.00      0.00        42\n",
      "     disgust       0.00      0.00      0.00       155\n",
      "        fear       0.00      0.00      0.00        11\n",
      "         joy       0.51      0.41      0.45       106\n",
      "        love       0.31      0.78      0.45       119\n",
      "     neutral       0.45      0.67      0.54       132\n",
      "    optimism       0.45      0.06      0.11        84\n",
      "   pessimism       0.00      0.00      0.00        39\n",
      "     sadness       0.33      0.07      0.12        67\n",
      "    surprise       0.00      0.00      0.00        29\n",
      "\n",
      "    accuracy                           0.44      1192\n",
      "   macro avg       0.24      0.25      0.20      1192\n",
      "weighted avg       0.33      0.44      0.33      1192\n",
      "\n",
      "Accuracy: 0.4388\n",
      "Precision: 0.3331\n",
      "Recall: 0.4388\n",
      "F1-Score: 0.3304\n",
      "\n",
      "==================================================\n",
      "OFFENSIVE LANGUAGE DETECTION\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.85      0.88      0.87       863\n",
      "         yes       0.66      0.60      0.63       329\n",
      "\n",
      "    accuracy                           0.80      1192\n",
      "   macro avg       0.76      0.74      0.75      1192\n",
      "weighted avg       0.80      0.80      0.80      1192\n",
      "\n",
      "Accuracy: 0.8037\n",
      "Precision: 0.7994\n",
      "Recall: 0.8037\n",
      "F1-Score: 0.8011\n",
      "\n",
      "==================================================\n",
      "HATE SPEECH DETECTION\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        hate       0.00      0.00      0.00        62\n",
      "    not_hate       0.95      1.00      0.97      1130\n",
      "\n",
      "    accuracy                           0.95      1192\n",
      "   macro avg       0.47      0.50      0.49      1192\n",
      "weighted avg       0.90      0.95      0.92      1192\n",
      "\n",
      "Accuracy: 0.9480\n",
      "Precision: 0.8987\n",
      "Recall: 0.9480\n",
      "F1-Score: 0.9227\n",
      "\n",
      "================================================================================\n",
      "METRICS SUMMARY TABLE\n",
      "================================================================================\n",
      "Task                 Accuracy     Precision    Recall       F1-Score    \n",
      "--------------------------------------------------------------------------------\n",
      "Emotion              0.4388       0.3331       0.4388       0.3304      \n",
      "Offensive            0.8037       0.7994       0.8037       0.8011      \n",
      "Hate                 0.9480       0.8987       0.9480       0.9227      \n",
      "--------------------------------------------------------------------------------\n",
      "AVERAGE              0.7301       0.6770       0.7301       0.6847      \n",
      "================================================================================\n",
      "\n",
      "Test predictions saved to 'test_predictions_with_original_data.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Usage Examples\n",
    "# ======================\n",
    "\n",
    "# Training Example:\n",
    "\n",
    "# To train the model:\n",
    "training_results = train_arabic_classifier(\n",
    "    data_file_path='/content/sample_data/train.csv',  # Replace with your training file\n",
    "    num_epochs=1,\n",
    "    batch_size=16,\n",
    "    learning_rate=2e-5\n",
    ")\n",
    "\n",
    "# Extract components\n",
    "classifier = training_results['classifier']\n",
    "model = training_results['model']\n",
    "test_loader = training_results['test_loader']\n",
    "test_df = training_results['test_df']\n",
    "\n",
    "# Evaluate on test set\n",
    "evaluation_results = evaluate_model(classifier, model, test_loader, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45416,
     "status": "ok",
     "timestamp": 1750333110794,
     "user": {
      "displayName": "Fatema Khatun",
      "userId": "04694556593640634304"
     },
     "user_tz": -180
    },
    "id": "IyPXs3EM8Svx",
    "outputId": "1edb2e8b-70da-4c14-aa55-8783594ba598"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading external test file: /content/sample_data/validation.csv\n",
      "External test file loaded with 1277 samples\n",
      "Columns in test file: ['id', 'text', 'Emotion', 'Offensive', 'Hate']\n",
      "Ground truth labels found. Will calculate accuracy metrics.\n",
      "Warning: Some labels in test file are not seen during training: y contains previously unseen labels: nan\n",
      "Proceeding with prediction only...\n",
      "Making predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1277/1277 [00:45<00:00, 28.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions saved to 'external_test_predictions_validation.xlsx'\n",
      "\n",
      "================================================================================\n",
      "SAMPLE PREDICTIONS FROM EXTERNAL TEST FILE\n",
      "================================================================================\n",
      "\n",
      "Example 1:\n",
      "Text: RT @fayz35510671: ÿ∑Ÿäÿ® ŸàŸäŸÜ ÿßŸÑŸÑŸä ŸÖÿ™ŸÅÿßÿπŸÑŸäŸÜ ŸÜÿ®Ÿä ŸÜŸÉŸÖŸÑ Ÿ•Ÿ†Ÿ†\n",
      "PREDICTIONS -> Emotion: love (conf: 0.200)\n",
      "               Offensive: no (conf: 0.941)\n",
      "               Hate: not_hate (conf: 0.985)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Text: Ÿäÿßÿ±Ÿäÿ™ ŸÜŸàŸÅÿ± ŸÇŸàÿ™ŸÜÿß Ÿàÿßÿ™ÿ≠ÿßÿØŸÜÿß ŸÅŸä Ÿáÿßÿ¥ÿ™ÿßÿ¨ ÿ≠ŸÇ ÿßŸÑÿÆÿ∑Ÿäÿ® ŸàŸÖŸäŸÜ ÿ®Ÿäÿ≠ŸÖŸä ÿßŸÑÿ≤ŸÑŸÜÿ∑ÿ≠Ÿä ÿßŸÜÿ™Ÿàÿß ÿ®ÿ™ŸÖÿ´ŸÑŸàÿß ÿπŸÑŸäŸÜÿß ŸÑÿ≥Ÿá ÿßŸÑÿÆÿ∑Ÿäÿ® ŸÖÿÆÿØ...\n",
      "PREDICTIONS -> Emotion: anger (conf: 0.616)\n",
      "               Offensive: yes (conf: 0.741)\n",
      "               Hate: not_hate (conf: 0.914)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "Text: ŸÑŸà ÿßŸÑÿ∫ŸÄŸÄŸÄŸÑÿß ŸäŸàÿÆÿ∞ ŸàŸäÿπÿ∑Ÿä ÿπŸÑŸâ ÿßŸÑŸÉŸäŸÅ<LF>ÿ™ÿ±Ÿâ ÿßŸÑÿ∫ŸÄŸÄŸÄŸÄŸÄŸÑÿß ŸÖŸÜŸÄŸÄŸÄŸÄŸÄŸä ŸÑŸÉ ÿßŸàŸÑ ŸáŸÄŸÄÿØŸäŸá<LF><LF>Ÿáÿ±ÿ¨Ÿä ŸÖÿπŸÉ ÿµÿßÿØŸÇ ŸàŸÑÿß ÿß...\n",
      "PREDICTIONS -> Emotion: love (conf: 0.157)\n",
      "               Offensive: no (conf: 0.913)\n",
      "               Hate: not_hate (conf: 0.982)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 4:\n",
      "Text: @Hamza_tekin2023 ÿßŸÑŸÉŸäÿßŸÜ ÿßŸÑÿµŸáŸäŸàŸÜŸä ŸÇÿ™ŸÑ Ÿàÿ∞ÿ®ÿ≠ ÿßŸÑŸÅŸÑÿ≥ÿ∑ŸäŸÜŸäŸäŸÜ Ÿàÿßÿπÿ™ÿØŸâ ÿπŸÑŸâ ÿßŸÑÿ≠ÿ±ŸàŸÖÿßÿ™ Ÿà ÿßŸÑÿßÿ±ÿßÿ∂Ÿä ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ŸÅŸä ŸÑÿ®ŸÜÿßŸÜ...\n",
      "PREDICTIONS -> Emotion: anger (conf: 0.585)\n",
      "               Offensive: yes (conf: 0.717)\n",
      "               Hate: not_hate (conf: 0.871)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 5:\n",
      "Text: ÿ™ÿ®ÿØÿ£ 23 ÿ£ÿ∫ÿ≥ÿ∑ÿ≥<LF>Ÿàÿ≤ÿßÿ±ÿ© ÿßŸÑÿ™ÿπŸÑŸäŸÖ ÿ™ÿπŸÑŸÜ ŸÖŸàÿßÿπŸäÿØ ÿßÿÆÿ™ÿ®ÿßÿ±ÿßÿ™ ÿßŸÑÿØŸàÿ± ÿßŸÑÿ´ÿßŸÜŸä<LF><LF>#ÿ¨ÿ±ŸäÿØÿ©_ÿßŸÑÿ±ÿßŸäÿ© #ŸÑÿ£ÿ¨ŸÑ_ŸÇÿ∑ÿ±_ŸÉŸÑŸÜÿß...\n",
      "PREDICTIONS -> Emotion: neutral (conf: 0.430)\n",
      "               Offensive: no (conf: 0.914)\n",
      "               Hate: not_hate (conf: 0.958)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Predict on external test file\n",
    "external_results = predict_on_external_test_file(\n",
    "    classifier=classifier,\n",
    "    model=model,\n",
    "    test_file_path='/content/sample_data/validation.csv'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNhDdhdr1X3Smq/aC7xu5sf",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
